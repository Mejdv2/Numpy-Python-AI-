{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mejdv2/Numpy-Python-AI-/blob/main/NumpyAi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8hMHixZKDUc"
      },
      "source": [
        "This Imports the Essential Libraries and some other functions (such as activation functions and their derivatives, cost functions, etc)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ffbnJHZ81K-Y",
        "outputId": "0eda1e60-2dc6-4a2c-d2d9-0781ad0b7fdb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#\"\"\"Activation Functions\"\"\"\n",
        "\n",
        "\n",
        "class SigmoidActFunc():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def Activation(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def Derivative(self, z):\n",
        "        return self.Activation(z) * (1 - self.Activation(z))\n",
        "\n",
        "\n",
        "class TanHActFunc():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def Activation(self, z):\n",
        "        e2z = np.exp(2 * z)\n",
        "        return (e2z - 1) / (e2z + 1)\n",
        "\n",
        "    def Derivative(self, z):\n",
        "        a = self.Activation(z)\n",
        "        return 1 - a **2\n",
        "\n",
        "\n",
        "class SiLUActFunc():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def Activation(self, z):\n",
        "        return z / (1 + np.exp(-z))\n",
        "\n",
        "    def Derivative(self, z):\n",
        "        sig = 1 / (1 + np.exp(-z))\n",
        "        return z * sig * (1 - sig) + sig\n",
        "\n",
        "\n",
        "\n",
        "class LinActFunc():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def Activation(self, z):\n",
        "        return z\n",
        "\n",
        "    def Derivative(self, z):\n",
        "        return np.ones(z.shape)\n",
        "\n",
        "\n",
        "\n",
        "class ReLUActFunc():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def Activation(self, z):\n",
        "\n",
        "        return np.maximum(z, np.zeros(z.shape))\n",
        "\n",
        "    def Derivative(self, z):\n",
        "        FlatZ = z.flatten()\n",
        "        ToRet = np.zeros(FlatZ.shape)\n",
        "\n",
        "        for zEl in range(z.size):\n",
        "            if FlatZ[zEl] > 0:\n",
        "                ToRet[zEl] = 1\n",
        "\n",
        "        return ToRet.reshape(z.shape)\n",
        "\n",
        "\n",
        "\n",
        "class PReLUActFunc():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def Activation(self, z):\n",
        "\n",
        "        return np.maximum(z, z * 0.05)\n",
        "\n",
        "    def Derivative(self, z):\n",
        "        FlatZ = z.flatten()\n",
        "        ToRet = np.zeros(FlatZ.shape) + 0.05\n",
        "\n",
        "        for zEl in range(FlatZ.size):\n",
        "            if FlatZ[zEl] > 0:\n",
        "                ToRet[zEl] = 1\n",
        "\n",
        "        return ToRet.reshape(z.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SoftmaxActFunc():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def Activation(self, z):\n",
        "        expSum = np.sum(z, 1)\n",
        "\n",
        "        return np.array([z[i] / expSum[i] for i in range(len(expSum))])\n",
        "\n",
        "    def Derivative(self, z):\n",
        "        expSum   = np.sum(z, 1)\n",
        "        expIn    = z\n",
        "        nNpA = [(expIn[i] * expSum[i] - expIn[i] ** 2) / (expSum[i] ** 2) for i in range(len(expSum))]\n",
        "\n",
        "        return np.array(nNpA)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#\"\"\"ERROR FUNCTIONS\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class MeanSquaredErrorCostFunc():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def Cost(self, Output, Expected):\n",
        "        return (Output - Expected) **2\n",
        "\n",
        "    def Derivative(self, Output, Expected):\n",
        "        return 2 * (Output - Expected)\n",
        "    \n",
        "\n",
        "class CrossEntropy():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def Cost(predictions, targets):\n",
        "        \"\"\"\n",
        "        Computes cross entropy between targets (encoded as one-hot vectors)\n",
        "        and predictions. \n",
        "        Input: predictions (N, k) ndarray\n",
        "               targets (N, k) ndarray        \n",
        "        Returns: scalar\n",
        "        \"\"\"\n",
        " \n",
        "        ce = -np.sum(targets*np.log2(predictions+1e-12))\n",
        "        return ce\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#\"\"\"TRAINING FUNCTIONS\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SGD():\n",
        "    def __init__(self, LrW, LrB, Momentum = 0.0):\n",
        "        self.LrWeight = LrW\n",
        "        self.LrBias   = LrB\n",
        "        self.Momentum = Momentum\n",
        "\n",
        "    def CalcGrad(FSelf, self, PrevLayer):\n",
        "        self.Backproped      = np.array([self.weights.dot(self.NodeValues[0])])\n",
        "\n",
        "        self.WeightGradiant += np.dot(PrevLayer.Activations.T, self.NodeValues)\n",
        "        self.BiasGradiant   += np.sum(self.NodeValues, 0)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def AppGrad(FSelf, self, DataSize):\n",
        "        self.biases  -= self.BiasGradiant   * (FSelf.LrBias / DataSize)\n",
        "        self.weights -= self.WeightGradiant * (FSelf.LrWeight / DataSize)\n",
        "\n",
        "        self.biases  -= self.BiasOld * FSelf.Momentum\n",
        "        self.weights -= self.WeightOld   * FSelf.Momentum\n",
        "\n",
        "        self.WeightOld       = self.WeightGradiant * (FSelf.LrWeight / DataSize)\n",
        "        self.BiasOld         = self.BiasGradiant   * (FSelf.LrBias / DataSize)\n",
        "\n",
        "        self.BiasGradiant    = np.zeros((1, self.NumOutNodes))\n",
        "        self.WeightGradiant  = np.zeros((self.NumInNodes, self.NumOutNodes))\n",
        "\n",
        "        return self\n",
        "\n",
        "Sigmoid = SigmoidActFunc()\n",
        "Linear  = LinActFunc()\n",
        "PReLU   = PReLUActFunc()\n",
        "ReLU    = ReLUActFunc()\n",
        "TanH    = TanHActFunc()\n",
        "SiLU    = SiLUActFunc()\n",
        "Softmax = SoftmaxActFunc()\n",
        "\n",
        "MeanSquaredError = MeanSquaredErrorCostFunc()\n",
        "\n",
        "\n",
        "x = Sigmoid.Activation(np.array([x/100 for x in range(-500, 500, 1)]))\n",
        "y = Sigmoid.Derivative(np.array([x/100 for x in range(-500, 500, 1)]))\n",
        "\n",
        "plt.plot(x)\n",
        "plt.plot(y)\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUfbHOQnbFC2"
      },
      "source": [
        "Imports tensorflow to load mnist dataset. then unload it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGFXaOk-a4Lu",
        "outputId": "8db5d7ff-613f-4d35-c11b-18d7c50b8c33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 784)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "assert x_train.shape == (60000, 28, 28)\n",
        "assert x_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)/255\n",
        "x_test  = x_test.reshape(10000, 784)/255\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "del tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "098ane3CcQ_T"
      },
      "source": [
        "Next is formatting the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tb_vrewFcdP7"
      },
      "outputs": [],
      "source": [
        "convertedY = np.zeros((60000, 10))\n",
        "\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "    currUpdate = y_train[i]\n",
        "    toOut = np.zeros(10)\n",
        "    toOut[currUpdate] = 1\n",
        "    convertedY[i] = toOut\n",
        "\n",
        "y_train = convertedY[0:60000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fS6HznLjhF6_"
      },
      "outputs": [],
      "source": [
        "convertedY = np.zeros((10000, 10))\n",
        "\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    currUpdate = y_test[i]\n",
        "    toOut = np.zeros(10)\n",
        "    toOut[currUpdate] = 1\n",
        "    convertedY[i] = toOut\n",
        "\n",
        "y_test = convertedY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1fJN2J4K7MY"
      },
      "source": [
        "Next is the layer class definition. We randomly define weights based off of the previous layer size and the current layer size. The -**self.Activations**- variable is used during feedforward to pass its values to the next layer. then we use it with  the **np.dot** function to feedforward it with the current weights, After that bias is added then the variable is activated and passed and so on.\n",
        "\n",
        "****NOTE:**** The first layer (Input Layer) does not dot its activation with the weights because it has no weights nor biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zIarRF-rlLfT"
      },
      "outputs": [],
      "source": [
        "class Layer():\n",
        "    def __init__(self, NumNodes, Activation, BackProper):\n",
        "        self.NumOutNodes    = NumNodes\n",
        "        self.NumInNodes     = 1\n",
        "\n",
        "        self.ActFunc        = Activation.Activation\n",
        "        self.DerFunc        = Activation.Derivative\n",
        "\n",
        "        self.BPCalc         = BackProper\n",
        "        self.WeightOld      = np.zeros((self.NumInNodes, NumNodes))\n",
        "        self.BiasOld        = np.zeros((1, NumNodes))\n",
        "\n",
        "\n",
        "        self.Activations    = np.zeros((1, NumNodes))\n",
        "        self.WeightedOut    = np.zeros((1, NumNodes))\n",
        "        self.Backproped     = np.zeros((1, NumNodes))\n",
        "        self.NodeValues     = np.zeros((1, NumNodes))\n",
        "        self.WeightGradiant = np.zeros((self.NumInNodes, NumNodes))\n",
        "        self.BiasGradiant   = np.zeros((1, NumNodes))\n",
        "\n",
        "        self.weights        = np.random.uniform(size = (self.NumInNodes, self.NumOutNodes)) / np.sqrt(self.NumInNodes)\n",
        "        self.biases         = np.zeros((1,              self.NumOutNodes))\n",
        "\n",
        "    def Update_InputDim(self, NumNodesIn):\n",
        "        self.NumInNodes = NumNodesIn\n",
        "\n",
        "        \n",
        "        self.weights = np.random.uniform(size = (self.NumInNodes, self.NumOutNodes)) / np.sqrt(self.NumInNodes)\n",
        "        self.WeightGradiant = np.zeros((self.NumInNodes, self.NumOutNodes))\n",
        "\n",
        "    def FeedForward(self, PrevLayer):\n",
        "        PrevLayerAct = PrevLayer.Activations\n",
        "\n",
        "        WeightedOut = np.dot(PrevLayerAct, self.weights)\n",
        "        InactiveOut = np.add(WeightedOut,  self.biases)\n",
        "\n",
        "        ActOutPut   = self.ActFunc(InactiveOut)\n",
        "\n",
        "        self.WeightedOut = InactiveOut\n",
        "        self.Activations = ActOutPut\n",
        "\n",
        "\n",
        "    #Now It's Time For BACKPROPOGATION!\n",
        "\n",
        "\n",
        "    def BackPropLayer(self, PrevLayer, NextLayer):\n",
        "        self.NodeValues  = NextLayer.Backproped\n",
        "        self.NodeValues *= self.DerFunc(self.WeightedOut)\n",
        "\n",
        "        self = self.BPCalc.CalcGrad(self, PrevLayer)\n",
        "\n",
        "    def ApplyGradiants(self, DataSize):\n",
        "        self = self.BPCalc.AppGrad(self, DataSize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82S9nKZvd-x0"
      },
      "source": [
        "In the above script, you can see that we need the next layer. This is because in each layer, we calculate the previous layer's node values. The weights' gradiant is adjusted by the previous layers activation (for input layer that's just the input) multiplied by the new node values. The bias's gradiant on the other hand are adjusted based off of the new node values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ofl3yZnlN-Hh"
      },
      "outputs": [],
      "source": [
        "class OutputLayer():\n",
        "    def __init__(self, NumNodes, Activation, Cost, BackProper):\n",
        "        self.NumOutNodes    = NumNodes\n",
        "        self.NumInNodes     = 1\n",
        "\n",
        "        self.ActFunc        = Activation.Activation\n",
        "        self.DerFunc        = Activation.Derivative\n",
        "\n",
        "        self.CostFunc       = Cost.Cost\n",
        "        self.CostDerivative = Cost.Derivative\n",
        "\n",
        "        self.BPCalc         = BackProper\n",
        "        self.WeightOld      = np.zeros((self.NumInNodes, NumNodes))\n",
        "        self.BiasOld        = np.zeros((1, NumNodes))\n",
        "\n",
        "        self.Activations    = np.zeros((1, NumNodes))\n",
        "        self.WeightedOut    = np.zeros((1, NumNodes))\n",
        "        self.Backproped     = np.zeros((1, NumNodes))\n",
        "        self.NodeValues     = np.zeros((1, NumNodes))\n",
        "        self.BiasGradiant   = np.zeros((1, NumNodes))\n",
        "\n",
        "        \n",
        "        self.weights     = np.random.uniform(size = (self.NumInNodes, self.NumOutNodes)) / np.sqrt(self.NumInNodes)\n",
        "        self.biases         = np.zeros((1, self.NumOutNodes))\n",
        "\n",
        "    def Update_InputDim(self, NumNodesIn):\n",
        "        self.NumInNodes = NumNodesIn\n",
        "\n",
        "        self.weights     = np.random.uniform(size = (self.NumInNodes, self.NumOutNodes)) / np.sqrt(self.NumInNodes)\n",
        "        self.WeightGradiant = np.zeros((self.NumInNodes, self.NumOutNodes))\n",
        "        self.biases         = np.zeros((1, self.NumOutNodes))\n",
        "\n",
        "    def FeedForward(self, PrevLayer):\n",
        "        PrevLayerAct = PrevLayer.Activations\n",
        "\n",
        "        WeightedOut = np.dot(PrevLayerAct, self.weights)\n",
        "        InactiveOut = np.add(WeightedOut,  self.biases)\n",
        "\n",
        "        ActOutPut   = self.ActFunc(InactiveOut)\n",
        "\n",
        "        self.WeightedOut = InactiveOut\n",
        "        self.Activations = ActOutPut\n",
        "\n",
        "\n",
        "    #Now It's Time For BACKPROPOGATION!\n",
        "\n",
        "\n",
        "    def CalculateNodeValues(self, ExpectedOut):\n",
        "        CurrentOut     = self.Activations\n",
        "        WeightedOutput = self.WeightedOut\n",
        "\n",
        "        CostDerivative   = self.CostDerivative(CurrentOut, ExpectedOut)\n",
        "        OutputDerivative = self.DerFunc(WeightedOutput)\n",
        "\n",
        "        NodeValue = CostDerivative * OutputDerivative\n",
        "        return NodeValue\n",
        "\n",
        "    def BackPropOutput(self, ExpectedOutput, PrevLayer):\n",
        "        self.NodeValues      = self.CalculateNodeValues(ExpectedOutput)\n",
        "        self = self.BPCalc.CalcGrad(self, PrevLayer)\n",
        "\n",
        "    def ApplyGradiants(self, DataSize):\n",
        "        self = self.BPCalc.AppGrad(self, DataSize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq1f1L15bN0V"
      },
      "source": [
        "As you might have noticed in the \n",
        "Backpropagation/Gradiant Descent section, we use a cost/loss function to calculate the error. this is to figure out how much we should change the node values and how much of an effect does each weight has on the error and how to decrease the error. The new value each of the weights' gradiant is equal to the new node values multiplied by the previous layers activations. The bias's gradiant is adjusted based off of the new node values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VpEJ-uwPITBa"
      },
      "outputs": [],
      "source": [
        "class InputLayer():\n",
        "    def __init__(self, NumNodes):\n",
        "        self.NumOutNodes = NumNodes\n",
        "\n",
        "        self.Activations = np.zeros((1, NumNodes))\n",
        "\n",
        "    def ApplyGradiants(self, DataSize):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yrLV12Im88F8"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork():\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def Add_Layer(self, Layer):\n",
        "        LayerAdd = Layer\n",
        "\n",
        "        if len(self.layers) > 0:\n",
        "            LayerAdd.Update_InputDim(self.layers[-1].NumOutNodes)\n",
        "\n",
        "        self.layers.append(LayerAdd)\n",
        "\n",
        "    def Predict(self, Input):\n",
        "        if len(self.layers) < 1:\n",
        "            raise ValueError(\"No Layers Found In Neural Network!\")\n",
        "\n",
        "        if len(self.layers) > 0:\n",
        "            self.layers[0].Activations = Input\n",
        "\n",
        "        if len(self.layers) > 1:\n",
        "            for LayerIndex in range(1, len(self.layers), 1):\n",
        "                self.layers[LayerIndex].FeedForward(self.layers[LayerIndex - 1])\n",
        "\n",
        "        return self.layers[-1].Activations\n",
        "\n",
        "    def BackProp(self, Inputs, Outputs):\n",
        "        if len(self.layers) < 2:\n",
        "            raise ValueError(\"Not Enough Layers In Neural Network! (Two Required for Training!)\")\n",
        "\n",
        "        self.Predict(Inputs)\n",
        "\n",
        "        self.layers[-1].BackPropOutput(Outputs, self.layers[-2])\n",
        "\n",
        "        if len(self.layers) > 2:\n",
        "            for LayerIndex in range(1, len(self.layers) - 1, -1):\n",
        "                self.layers[LayerIndex].BackPropLayer(self.layers[LayerIndex - 1], self.layers[LayerIndex + 1])\n",
        "\n",
        "    def ApplyAllGradiants(self, DatasetSize):\n",
        "        for i in range(len(self.layers)):\n",
        "            self.layers[i].ApplyGradiants(DatasetSize)\n",
        "\n",
        "    def CalcCost(self, Input, ExpectedOutput):\n",
        "        CostFunc = self.layers[-1].CostFunc\n",
        "\n",
        "        Output = self.Predict(Input)\n",
        "        return np.sum(CostFunc(Output, ExpectedOutput))\n",
        "\n",
        "    def CalcCosts(self, Inputs, ExpectedOutputs):\n",
        "        Costs = [self.CalcCost(Input, Output) for Input, Output in zip(Inputs, ExpectedOutputs)]\n",
        "\n",
        "        return np.sum(Costs) / len(Inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK-eBDLvUbAB"
      },
      "source": [
        "****Test Time!****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfhfJTP0Ulta",
        "outputId": "29ee445c-be51-4137-ea1f-65d89de513c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.59798467 0.58995246]\n",
            " [0.61574093 0.60467048]\n",
            " [0.61360432 0.6033528 ]\n",
            " [0.6199483  0.60833722]\n",
            " [0.62217114 0.6101578 ]\n",
            " [0.62637205 0.6135757 ]\n",
            " [0.62605381 0.61332954]\n",
            " [0.6267148  0.61388084]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "TM = SGD(0.5, 0.2, 0)\n",
        "\n",
        "Neural = NeuralNetwork()\n",
        "Neural.Add_Layer(InputLayer(2))\n",
        "\n",
        "Neural.Add_Layer(Layer(3, TanH, TM))\n",
        "Neural.Add_Layer(Layer(3, TanH, TM))\n",
        "\n",
        "Neural.Add_Layer(OutputLayer(2, Sigmoid, MeanSquaredError, TM))\n",
        "\n",
        "InputsX  = np.array([[1, 1], [1, 2], [2, 1], [1.8, 1.8],\n",
        " [2, 2], [2.4, 3.2], [2.6, 2.8], [3, 3]])\n",
        "OutputsY = np.array([[0, 1], [0, 1], [0, 1], [0, 1],\n",
        "     [1, 0], [1, 0],   [1, 0], [1, 0]])\n",
        "\n",
        "print(Neural.Predict(InputsX))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1OA-iFVTl98",
        "outputId": "55f050d0-a6a6-4c73-fc1d-b649d6c59720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09268102178087703\n",
            "0.07269924238410425\n"
          ]
        }
      ],
      "source": [
        "print(Neural.CalcCosts(InputsX, OutputsY))\n",
        "\n",
        "for i in range(100000):\n",
        "    Neural.BackProp(InputsX, OutputsY)\n",
        "    Neural.ApplyAllGradiants(8)\n",
        "    \n",
        "print(Neural.CalcCosts(InputsX, OutputsY))\n",
        "\n",
        "x = np.argmax(Neural.Predict(InputsX), 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWt_KTghcRxM",
        "outputId": "da301b07-1ca3-45e0-c493-8fe647b9d06c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 0 0 0 0]\n",
            "[[2.57667940e-05 9.99974164e-01]\n",
            " [6.66980357e-02 9.33284026e-01]\n",
            " [3.53816849e-02 9.64581979e-01]\n",
            " [3.81510975e-01 6.18459351e-01]\n",
            " [6.44320836e-01 3.55694831e-01]\n",
            " [9.34375846e-01 6.56491515e-02]\n",
            " [9.24611483e-01 7.54142954e-02]\n",
            " [9.45008069e-01 5.50129028e-02]]\n"
          ]
        }
      ],
      "source": [
        "print(x)\n",
        "\n",
        "print(Neural.Predict(InputsX))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1A7nCERiSHm"
      },
      "source": [
        "Now It's Time For the Big Test!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL44heeTI64T",
        "outputId": "89db2d2e-c211-4a23-8e69-f241445a1af8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100000, 1)\n",
            "12.966660253851986\n",
            "5.21165887414919e-31\n",
            "[[2.55]]\n",
            "[[3.]]\n",
            "[[0.45]]\n"
          ]
        }
      ],
      "source": [
        "TM = SGD(0.2, 0.2, 0)\n",
        "\n",
        "np.random.seed(4)\n",
        "\n",
        "nn = NeuralNetwork()\n",
        "\n",
        "nn.Add_Layer(InputLayer(1))\n",
        "\n",
        "nn.Add_Layer(OutputLayer(1, Linear, MeanSquaredError, TM))\n",
        "\n",
        "\n",
        "x = np.random.normal(size=(100000, 1))\n",
        "\n",
        "print(x.shape)\n",
        "\n",
        "y = x * 2.55 + 3\n",
        "\n",
        "print(nn.CalcCosts(x, y))\n",
        "\n",
        "for i in range(10000):\n",
        "    nn.BackProp(x, y)\n",
        "    nn.ApplyAllGradiants(100000)\n",
        "\n",
        "print(nn.CalcCosts(x, y))\n",
        "\n",
        "print(nn.layers[-1].weights)\n",
        "print(nn.layers[-1].biases)\n",
        "\n",
        "print(nn.Predict(-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discard The Rest, i havent implemented Cross Entropy and A descent softmax yet."
      ],
      "metadata": {
        "id": "uddtqG2WAdPt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsszGfkHiPZO"
      },
      "outputs": [],
      "source": [
        "NeuralT = NeuralNetwork()\n",
        "NeuralT.Add_Layer(InputLayer(784))\n",
        "\n",
        "NeuralT.Add_Layer(Layer(50, Sigmoid, 5, 0.25))\n",
        "# NeuralT.Add_Layer(Layer(216, Sigmoid, 0.5, 0.0025))\n",
        "\n",
        "NeuralT.Add_Layer(OutputLayer(10, Sigmoid, MeanSquaredError, 5, 0.25))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r6vTtpfjzVd"
      },
      "outputs": [],
      "source": [
        "print(NeuralT.CalcCosts(x_train, y_train))\n",
        "\n",
        "for i in range(10):\n",
        "    NeuralT.BackProp(x_train, y_train)\n",
        "    NeuralT.ApplyAllGradiants(10000)\n",
        "    \n",
        "\n",
        "\n",
        "print(NeuralT.CalcCosts(x_train, y_train))\n",
        "\n",
        "print(np.argmax(NeuralT.Predict(x_train[1])))\n",
        "\n",
        "print(np.argmax(y_train[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UW3F0FywRMa"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(np.argmax(NeuralT.Predict(x_train[10])))\n",
        "\n",
        "print(np.argmax(y_train[10]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwx2Cvdv7isWxL+VOVC870",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}